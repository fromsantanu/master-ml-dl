# **Lesson 70: Vision Transformers (ViT — Simple Introduction)**

*How Transformers (the same technology behind ChatGPT) are now used for images*

Transformers changed the world of language models — they power ChatGPT, Google Bard, and many modern AI tools.
But recently, researchers discovered something surprising:

### **Transformers also work extremely well for images!**

These models are called **Vision Transformers (ViT)**.

Don’t worry — we will explain this in very simple words.

---

# **1. The Big Idea: How do Vision Transformers see images?**

CNNs look at images using **filters** to find edges and shapes.
Transformers do something different.

They break the image into **small patches**, like cutting a big photo into tiny square pieces.

Example:

A 224×224 image is cut into 16×16 patches → around 196 small squares.

Each patch becomes like a “word” in a sentence.

So instead of reading text tokens, the transformer reads **image tokens**.

---

# **2. Simple Analogy**

Imagine cutting a photo into many small tiles and giving each tile a number (a code):

* Tile 1 = top-left corner
* Tile 2 = top-middle
* Tile 3 = top-right
* …
* Tile 196 = bottom-right corner

The model then studies how all tiles relate to each other.

This is exactly how transformers work:
They understand **relationships** between pieces.

---

# **3. Why Transformers are powerful for images**

Transformers use a special ability called **attention**.

### **Attention means:**

The model decides which parts of the image are important and looks at them more.

For example, in a picture of a cat sitting on a carpet:

* The cat's face will get more attention
* The background carpet will get less attention

This helps the model focus on meaningful parts.

---

# **4. CNN vs Vision Transformer (simple comparison)**

| Feature  | CNN                                         | ViT                                    |
| -------- | ------------------------------------------- | -------------------------------------- |
| Looks at | Local patterns (small area)                 | Global patterns (whole image together) |
| Learns   | Slowly builds from edges → shapes → objects | Looks at all patches at once           |
| Strength | Very good for small datasets                | Very powerful for large datasets       |
| Weakness | Needs convolution layers                    | Needs a lot of data to work well       |
| Speed    | Fast to train                               | Sometimes slower                       |

But when trained properly, ViTs often give **higher accuracy** than CNNs.

---

# **5. How a Vision Transformer works (simple steps)**

Let’s keep this super easy:

### **Step 1: Split the image into patches**

Like cutting the image into small squares.

### **Step 2: Convert each patch into numbers**

Each patch becomes a vector (like a representation).

### **Step 3: Add positional information**

So the model knows the patch’s location.
(This is like saying “this tile came from the top-left corner.”)

### **Step 4: Send all patches into a Transformer**

The transformer uses **attention** to understand:

* Which parts are important
* How patches relate to each other
* What the overall object might be

### **Step 5: Output final class**

For example:

* Cat
* Dog
* Car
* Mango

---

# **6. Real-Life Example**

Imagine you are solving a jigsaw puzzle:

* You look at each piece
* You compare it with other pieces
* You see how pieces fit together
* You slowly understand the full picture

Vision Transformers do something similar.

---

# **7. Why ViTs became popular**

Because they offer:

* Very high accuracy
* Ability to capture long-range relationships
* Less need for complex convolution designs
* Better performance for large datasets
* Simpler architecture compared to deep CNNs

Google’s ViT model showed excellent results on ImageNet.

---

# **8. A tiny code example (just to give an idea)**

Using PyTorch (very simple):

```python
from torchvision.models import vit_b_16

model = vit_b_16(pretrained=True)

output = model(image_tensor)
```

This loads a pre-trained Vision Transformer.

---

# **9. Where Vision Transformers are used?**

* Medical imaging
* Satellite images
* Self-driving car perception
* Face recognition
* Industrial automation
* Surveillance
* Robotics
* Image search engines

Anywhere CNNs were used, ViTs are slowly taking over.

---

# **10. Summary (super simple words)**

* Vision Transformers (ViTs) use the same idea as language transformers.
* They cut images into small patches and treat each patch like a “word.”
* They use **attention** to decide which patches matter.
* ViTs often outperform CNNs, especially with large datasets.
* They are becoming the future of computer vision.

---

