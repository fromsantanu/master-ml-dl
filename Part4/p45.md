# **Lesson 45: Hyperparameter Tuning**

### *Finding the best settings for your ML model*

Hyperparameter tuning simply means **adjusting the settings of your ML model** to get the best performance.

Think of it like cooking pasta:
If the flame is too low â†’ pasta becomes sticky.
If the flame is too high â†’ pasta becomes overcooked.
You need the *right* temperature and *right* timing.

Similarly, ML models have settings that you must adjust to get the best results.

---

## **ğŸŸ¦ 1. What are Hyperparameters? (Simple meaning)**

Hyperparameters are **model settings** you choose *before* training.

Examples:

* How deep the decision tree should be
* How many trees a random forest should have
* Learning rate of a neural network
* Number of neighbors in KNN

These are not learned by the model.
**You** decide them.

Like choosing oven temperature before baking.

---

## **ğŸŸ¦ 2. Why tune hyperparameters?**

Because the default settings may not be the best.

If settings are wrong:

* Model may underfit (learns too little)
* Model may overfit (memorizes too much)
* Model may be slow or unstable

Tuning helps you find the â€œsweet spot.â€

---

## **ğŸŸ¦ 3. Simple Real-Life Example**

Imagine you have an AC remote.
You choose:

* temperature
* fan speed
* swing mode

If you set these correctly, you feel comfortable.
If not, you feel hot or cold.

Hyperparameter tuning works the same way.

---

## **ğŸŸ¦ 4. How to Tune Hyperparameters**

There are simple and popular methods.

---

### **âœ” Method 1: Manual Tuning (Trial and Error)**

This is the easiest.
You try different values and see what works.

Example for KNN:

* Try K = 3
* Try K = 5
* Try K = 7

Choose the best one.

Like trying different sugar levels in tea and choosing your favorite.

---

### **âœ” Method 2: Grid Search**

Here you prepare a **table of possible values**
and test every combination.

Example for Random Forest:

* number of trees: 50, 100, 150
* max depth: 5, 10

Grid Search tests:

* 50 & 5
* 50 & 10
* 100 & 5
* 100 & 10
* 150 & 5
* 150 & 10

Then it picks the best combination.

It is slow but very thorough.

---

### **âœ” Method 3: Random Search**

Instead of testing all combinations,
Random Search picks **random** combinations.

Much faster.

Real-life example:
If you want to taste new food in a buffet,
you donâ€™t try every dish.
You pick a few randomly and choose the best.

---

### **âœ” Method 4: Bayesian Optimization (Easy Meaning)**

This is a smarter technique.
It learns from previous attempts and chooses better next settings.

You can think of it like:

* tasting your sambar
* if it's too salty, you reduce salt next time
* you keep improving based on feedback

It saves time compared to Grid Search.

---

## **ğŸŸ¦ 5. Hyperparameter Tuning with Cross-Validation**

Usually, tuning is done using **cross-validation** so we get a reliable score.

For each combination:

* Train â†’ Test â†’ Score
* Repeat for all folds
* Pick the combination with the best average score

This prevents overfitting on the validation set.

---

## **ğŸŸ¦ 6. Examples of Hyperparameters**

### **Decision Tree**

* max_depth
* min_samples_split
* min_samples_leaf

### **Random Forest**

* n_estimators (number of trees)
* max_depth
* max_features

### **Logistic Regression**

* C (regularization strength)

### **KNN**

* k (number of neighbors)
* distance metric

---

## **ğŸŸ¦ 7. Summary**

Hyperparameter tuning means:

* Choosing the best model settings
* Trying different combinations
* Using methods like Grid Search or Random Search
* Validating choices with cross-validation

Just like adjusting the flame while cooking,
correct hyperparameters make your model work perfectly.

---

