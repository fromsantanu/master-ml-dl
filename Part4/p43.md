# **Lesson 43: Handling Imbalanced Data**

### *What to do when one class is much larger than the other*

Imbalanced data means **your groups or classes are not equal**.

Example:
Imagine you are predicting whether a person has a disease.
You have:

* **990 people healthy**
* **10 people sick**

This is like a classroom where:

* 99 students pass
* only 1 fails

The model will keep saying â€œhealthyâ€ or â€œpassâ€ because it sees these most often.

This creates a big problem.

---

## **ğŸŸ¦ 1. Why is imbalance a problem?**

Because the model becomes **lazy**.

It learns to always pick the majority class.

Example:
If you always say â€œhealthy,â€ you will be correct 990 times out of 1000 (99% accuracy).
But you will **miss all sick people**, which is dangerous.

So accuracy becomes misleading.

We must fix the imbalance before training the model.

---

## **ğŸŸ¦ 2. How to detect imbalanced data**

Very simple:
Count how many samples are in each class.

If the ratio is very uneven (like 90:10, 95:5, or 99:1),
your data is **imbalanced**.

---

## **ğŸŸ¦ 3. Techniques to handle Imbalanced Data**

Below are simple and practical ways to solve this issue.

---

### **âœ” Method 1: Oversampling (Increase the minority class)**

You make more copies of the smaller group.

Example:
You have only 10 sick people.
You can duplicate them to make 50 or 100.

Most popular method: **SMOTE**
(SMOTE creates *new* synthetic/imaginary samples, not just duplicates.)

Real-life example:
If only 2 students are from a rare background, you invite more students like them to balance the class.

---

### **âœ” Method 2: Undersampling (Reduce the majority class)**

Here you reduce the big group.

Example:
From 990 healthy people, you take only 200 (randomly).

Real-life example:
If your class has 100 noisy students and 10 quiet students,
you select only 20 noisy students to make the discussion fair.

**Good:** Faster training
**Bad:** You lose information.

---

### **âœ” Method 3: Class Weights (Give more importance to the minority class)**

You tell the model:

> â€œIf you misclassify the rare class, you will be punished more.â€

This forces the model to pay more attention to the minority group.

Real-life example:
If a teacher says:

> â€œIf someone misses the important chapter, you lose double marks,â€
> students pay more attention.

Most ML algorithms support class weights (like Logistic Regression, SVM, RandomForest).

---

### **âœ” Method 4: Collect More Data**

If possible, try to gather more samples from the minority class.

Example:
In medical research, try collecting more samples of rare disease cases.

This is the best solution, but not always easy.

---

## **ğŸŸ¦ 4. Evaluating Models on Imbalanced Data**

Accuracy is not useful.

You should use:

* Precision
* Recall
* F1-score
* ROC-AUC
* Confusion matrix

These metrics focus on how well the model catches the minority class.

Real-life example:
If you are looking for a *rare* bird species,
you donâ€™t measure success by how many *common* birds you saw.
You measure whether you caught the rare one.

---

## **ğŸŸ¦ 5. Simple Example**

Predict if a transaction is fraud.

Out of 10,000 transactions:

* 9,950 are normal
* 50 are fraud

Without fixing imbalance, the model says:

> â€œEverything is normal.â€

Accuracy = 99.5%
But it misses all the fraud cases.

Using oversampling + class weights helps detect more fraud transactions.

---

## **ğŸŸ¦ 6. Summary**

Imbalanced data means one class is much bigger than the other.
If not fixed, the model becomes biased toward the majority class.

You learned four easy techniques:

* Oversampling
* Undersampling
* Class weights
* Collecting more minority data

You also learned that accuracy is not a good metric here.

---
