# **Lesson 80: NLP Project â€” Build a Text Classifier Using Transformers**

*(Very simple, step-by-step explanation for beginners)*

---

# ğŸŒŸ **Project Goal (In Simple Words)**

We will build a small NLP project where the model reads a piece of text and predicts a label â€” for example:

* â€œThis movie is amazing!â€ â†’ **Positive**
* â€œThe service was terrible.â€ â†’ **Negative**

This is called **text classification**.

We will use a Transformer model (like **BERT**) because it understands language very well.

---

# ğŸŒŸ **What You Will Learn in This Project**

* How to prepare text data
* How to load a pre-trained transformer
* How to train it for classification
* How to test its predictions
* How to save and use the model

Everything will be explained in very small steps.

---

# ğŸŒŸ **Step 1: Install the Required Libraries**

We will use the **Hugging Face Transformers** library because it makes using BERT/GPT models very easy.

```bash
pip install transformers
pip install torch
pip install datasets
pip install scikit-learn
```

This installs:

* transformer models
* PyTorch (for model training)
* datasets helper
* accuracy calculation tools

---

# ğŸŒŸ **Step 2: Choose a Model**

We will use **BERT base**, a small but powerful transformer.

```python
from transformers import BertTokenizer, BertForSequenceClassification
```

---

# ğŸŒŸ **Step 3: Load the Tokenizer**

A tokenizer converts text â†’ numbers so the model can understand it.

```python
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
```

Example:
â€œGreat movie!â€ â†’ turns into numerical tokens.

---

# ğŸŒŸ **Step 4: Prepare Sample Training Data**

Letâ€™s create a tiny dataset just for learning.

```python
texts = [
    "I loved the movie!",
    "The food was terrible.",
    "Amazing experience",
    "I will never come back",
    "Great service",
    "Worst hotel ever"
]

labels = [1, 0, 1, 0, 1, 0]  # 1 = positive, 0 = negative
```

---

# ğŸŒŸ **Step 5: Tokenize the Text**

Transformers require tokenized inputs:

```python
encoding = tokenizer(
    texts,
    padding=True,
    truncation=True,
    return_tensors='pt'
)
```

This produces:

* input_ids
* attention_mask

Both are required by BERT.

---

# ğŸŒŸ **Step 6: Load the Pre-trained BERT Model**

```python
model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased',
    num_labels=2
)
```

We set `num_labels=2` because we are doing positive/negative classification.

---

# ğŸŒŸ **Step 7: Train the Model (Small Example)**

We will use PyTorch for training:

```python
import torch

optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)
loss_fn = torch.nn.CrossEntropyLoss()

inputs = encoding['input_ids']
masks = encoding['attention_mask']
labels = torch.tensor(labels)

model.train()

for epoch in  range(10):
    optimizer.zero_grad()
    outputs = model(inputs, attention_mask=masks, labels=labels)
    loss = outputs.loss
    loss.backward()
    optimizer.step()
    print("Epoch:", epoch, "Loss:", loss.item())
```

This is a small training loop just for demonstration.

In real projects, you will use:

* bigger datasets
* training batches
* validation sets

But the idea remains the same.

---

# ğŸŒŸ **Step 8: Test the Model**

Letâ€™s try predicting a new sentence:

```python
test_text = "I really liked the food."

test_encoding = tokenizer(
    test_text,
    return_tensors='pt',
    padding=True,
    truncation=True
)

model.eval()
output = model(**test_encoding)
prediction = torch.argmax(output.logits)

print("Prediction:", "Positive" if prediction == 1 else "Negative")
```

If the model learned well, it should say:

**Prediction: Positive**

---

# ğŸŒŸ **Step 9: Save the Model**

```python
model.save_pretrained("sentiment_model")
tokenizer.save_pretrained("sentiment_model")
```

Your trained model is now saved in a folder.

---

# ğŸŒŸ **Step 10: Load the Saved Model Anywhere**

```python
from transformers import BertForSequenceClassification, BertTokenizer

model = BertForSequenceClassification.from_pretrained("sentiment_model")
tokenizer = BertTokenizer.from_pretrained("sentiment_model")
```

Now you can use it inside:

* web apps
* chatbots
* mobile apps
* FastAPI services
* Flask apps

---

# ğŸŒŸ **What You Learned in This Project**

* How a transformer reads text
* How tokenization works
* How to train a transformer for classification
* How to predict using the trained model
* How to save and load models

This is the basic structure followed in almost all modern NLP projects.

---

# ğŸŒŸ **Summary (Very Simple Points)**

* We built a small **text classifier**
* Used **BERT**, a transformer that understands language very well
* Cleaned and converted text into numbers
* Trained the model
* Tested it
* Saved and reused it

---



