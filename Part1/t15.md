# **Lesson 15 — Understanding the Machine Learning Pipeline**

*End-to-End Engineering Workflow for Building ML Systems*

A Machine Learning pipeline is a **sequence of structured steps** that transforms raw data into a deployed predictive system.
Each step has specific goals, inputs, outputs, and dependencies.

This lesson provides a rigorous, engineering-level breakdown of the pipeline from start to finish.

---

# **1. Overview of the ML Pipeline**

An ML pipeline defines the following mapping:

[
\text{Raw Data} \xrightarrow{\text{Processing}}
\text{Features} \xrightarrow{\text{Model}}
\text{Prediction}
]

Formally, the pipeline is:

[
\mathcal{D} \xrightarrow{\text{EDA}} \xrightarrow{\text{Cleaning}}
\xrightarrow{\text{Feature Engineering}} \xrightarrow{\text{Training}}
\xrightarrow{\text{Evaluation}} \xrightarrow{\text{Deployment}}
]

---

# **2. Step-by-Step Breakdown**

## **2.1 Problem Definition**

Define:

* Input space (X)
* Output space (Y)
* Objective function
* Constraints (accuracy, latency, interpretability)

Example:
[
X = \text{patient features},\quad Y = \text{disease label}
]

---

## **2.2 Data Collection**

Gather data from:

* Databases, APIs, sensors, logs
* Files (CSV, JSON, Parquet)
* Real-time streams

Ensure that:

[
\mathcal{D} = {(X_i, Y_i)}_{i=1}^N \sim P(X,Y)
]

---

## **2.3 Data Cleaning**

Fix issues such as:

* Missing values
* Duplicates
* Outliers
* Type mismatches
* Inconsistencies

Goal: produce a cleaned dataset (\mathcal{D}^*).

---

## **2.4 Exploratory Data Analysis (EDA)**

Analyze:

* Distributions
* Correlations
* Feature relationships
* Class imbalance

Mathematically inspect:

[
\mu,, \sigma,, \text{Skewness},, \rho_{ij}
]

EDA informs the next steps (feature scaling, transformations, modeling choices).

---

## **2.5 Feature Engineering & Preprocessing**

Transform raw features into model-ready form.

### **Numerical Features**

* Scaling (Standardization, Min–Max)
* Transformations (log, sqrt)

### **Categorical Features**

* Encoding (one-hot, ordinal, target encoding)

### **Text Features**

* Tokenization
* Word embeddings

### **Image Features**

* Pixel normalization
* Data augmentation

Feature engineering defines the feature map:

[
\phi : X \rightarrow \mathbb{R}^d
]

Producing feature matrix:

[
\mathbf{X} \in \mathbb{R}^{N \times d}
]

---

## **2.6 Train/Test Split**

Split the data:

[
\mathcal{D}*{\text{train}},; \mathcal{D}*{\text{test}}
]

Guarantee:

* No leakage
* Balanced classes (stratification)
* Temporal order preserved (time-series)

---

## **2.7 Model Selection & Training**

Choose model family:

* Linear models
* Tree-based models
* SVM
* Neural networks
* Probabilistic models

Training minimizes empirical loss:

[
\theta^* =
\arg\min_\theta \frac{1}{n} \sum_{i \in \mathcal{D}*{\text{train}}}
L(\hat{f}*\theta(X_i), Y_i)
]

Gradient-based optimizers:

* SGD
* Adam
* RMSProp

Non-gradient-based:

* Random forests
* XGBoost
* k-NN

---

## **2.8 Validation & Hyperparameter Tuning**

Use:

* Validation set
* k-fold cross-validation

Goal: find hyperparameters (\lambda) minimizing validation error:

[
\lambda^* =
\arg\min_\lambda R_{\text{val}}(\theta_\lambda)
]

Examples of hyperparameters:

* Learning rate
* Number of trees
* Depth of tree
* Regularization strength
* Number of neurons/layers

---

## **2.9 Model Evaluation**

Evaluate on the **test set** using objective metrics.

### **Regression Metrics**

[
\text{MAE},\quad \text{MSE},\quad \text{RMSE},\quad R^2
]

### **Classification Metrics**

[
\text{Accuracy},\quad
\text{Precision},\quad
\text{Recall},\quad
F_1,\quad
\text{ROC-AUC}
]

Evaluation computes:

[
R_{\text{test}} =
\frac{1}{m} \sum_{i \in \mathcal{D}_{\text{test}}}
L(\hat{f}(X_i), Y_i)
]

Test set must be **untouched** during training and tuning to avoid optimistic bias.

---

## **2.10 Model Deployment**

Convert trained model into a production-ready component.

Deployment methods:

* REST API (FastAPI, Flask)
* Batch inference
* Edge devices
* Cloud ML services (SageMaker, Vertex AI)

Requirements:

* Latency constraints
* Monitoring hooks
* Explainability tools (for regulated domains)

---

## **2.11 Monitoring & Maintenance**

Monitor for:

* Data drift
* Concept drift
* Performance degradation
* Latency issues

Formally compare distribution shift:

[
D_{\text{KL}}(P_{\text{train}} \parallel P_{\text{live}})
]

Retrain model when shift exceeds threshold.

---

# **3. The ML Pipeline as a Directed Acyclic Graph (DAG)**

A pipeline can be visualized as a DAG:

```
Raw Data
   │
Cleaning ──> EDA ──> Feature Engineering
   │               │
Train/Test Split   │
        │          │
     Training ──> Hyperparameter Tuning
        │
    Evaluation
        │
    Deployment
        │
    Monitoring
```

Each component is modular, testable, and reusable.

---

# **4. Best Engineering Practices**

### **1. Separation of data transform phases**

* Fit scalers/encoders only on training data
* Apply transform to both train and test

### **2. Pipeline tools**

Use frameworks that enforce reproducibility:

* scikit-learn Pipelines
* MLflow
* Airflow
* Kubeflow

### **3. Versioning**

Version:

* Dataset
* Model
* Hyperparameters
* Code

### **4. Avoid leakage**

Never use future data or test data during training.

---

# **Summary**

An ML pipeline is an orchestrated sequence of:

1. Problem formulation
2. Data acquisition
3. Cleaning
4. EDA
5. Feature engineering
6. Train/test split
7. Model training
8. Hyperparameter tuning
9. Evaluation
10. Deployment
11. Monitoring

Each stage has specific mathematical and engineering responsibilities that collectively determine model performance, reliability, and maintainability.

---
