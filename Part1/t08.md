# **Lesson 8 — Data Cleaning (Missing Values, Duplicates, Outliers, Inconsistencies)**

Data cleaning is a critical step before applying any ML algorithm.
Most real-world datasets contain noise, missing entries, redundancies, and incorrect values.
Poor data quality directly increases model bias and variance.

This lesson covers the essential cleaning operations:

1. Handling missing values
2. Removing duplicates
3. Detecting and treating outliers
4. Fixing inconsistencies and formatting errors

---

# **1. Handling Missing Values**

Let a dataset contain features ( X_1, X_2, \ldots, X_d ) and observations (X_i \in \mathbb{R}^d).
A missing value occurs when some components of (X_i) are null or undefined.

Mathematically:
[
X_{ij} = \text{NaN}
]

Missingness types (Little & Rubin):

* **MCAR** — Missing Completely At Random
  (P(\text{missing})) independent of data
* **MAR** — Missing At Random
  depends only on observed data
* **MNAR** — Missing Not At Random
  depends on unobserved values → hardest to fix

---

## **1.1 Detection**

Compute missingness indicator for feature ( X_j ):

[
m_j = \frac{\text{count of NaN in } X_j}{n}
]

If ( m_j > 0.4 ) → consider dropping the feature.

---

## **1.2 Imputation Methods**

### **a) Simple Numerical Imputation**

* Mean:
  [
  X_{ij} \leftarrow \frac{1}{n_j} \sum_{k:X_{kj}\neq NaN} X_{kj}
  ]
* Median (robust to outliers)
* Mode (for discrete values)

**When useful:** low missing fraction, unimodal distributions.

---

### **b) Categorical Imputation**

Replace NaNs with:

* Most frequent category (mode)
* A special token: `"Unknown"`, `"Missing"`

---

### **c) KNN Imputation**

For each missing value in row (i):

1. Find k-nearest rows using distance over non-missing features
2. Impute using mean (for numerical) or mode (for categorical)

Useful when dataset has local structure.

---

### **d) Model-Based Imputation**

Fit a predictive model (e.g., regression, random forest) to estimate missing values:

[
X_{ij} \approx f_{\theta}(X_{i, -j})
]

Best for datasets with significant correlations between features.

---

## **1.3 When to Drop Rows or Columns**

Drop rows if:

[
\text{missing cells per row} > 50%
]

Drop columns if:

[
m_j = \frac{\text{missing in feature } j}{n} > 40%
]

---

# **2. Removing Duplicates**

Duplicate rows bias the model because repeated samples overweight certain patterns.

Let dataset contain rows:

[
X_i = X_j, \quad i \ne j
]

Then the empirical distribution is distorted:

[
\hat{P}(X=x) = \frac{\text{count}(x)}{n}
]

### **Detection**

Use hashing or row-wise equality checks.

### **Removal**

Keep only unique rows using:

* SQL: `SELECT DISTINCT`
* Pandas: `drop_duplicates()`
* Spark: `dropDuplicates()`

Duplicates should almost always be removed unless they represent valid repeated events.

---

# **3. Outlier Detection and Treatment**

Outliers are points where:

[
|x - \mu| \gg \sigma
]

or deviate significantly from expected patterns.

### **Why Outliers Matter**

* Distort mean and variance
* Mislead distance-based models (KNN, K-means)
* Reduce regression accuracy
* Affect scaling (min–max normalization)

---

## **3.1 Statistical Methods**

### **a) Z-score Method**

For feature (X):

[
z_i = \frac{X_i - \mu}{\sigma}
]

If:

[
|z_i| > 3
]

→ Outlier candidate.

---

### **b) IQR Method**

Let:

* (Q1) = 25th percentile
* (Q3) = 75th percentile
* (IQR = Q3 - Q1)

Outlier if:

[
X_i < Q1 - 1.5 \cdot IQR \quad \text{or} \quad X_i > Q3 + 1.5 \cdot IQR
]

Robust to skewed distributions.

---

## **3.2 Distance-Based Methods**

### **a) Mahalanobis Distance**

[
d_i = \sqrt{(X_i - \mu)^T \Sigma^{-1} (X_i - \mu)}
]

Compare with chi-square threshold
(for dimension (d)):

[
d_i^2 > \chi^2_{p=0.975, df=d}
]

### **b) Isolation Forest**

Algorithm isolates anomalies via recursive partitions.
Good for high-dimensional data.

---

## **3.3 Outlier Treatment**

Options:

1. **Cap/floor** values at percentile limits (winsorization)
2. **Transform** (log, sqrt) to reduce skew
3. **Remove** outliers if they represent measurement error
4. **Model-specific handling** (robust regression)

---

# **4. Fixing Inconsistencies and Formatting Errors**

These issues do not break the data structurally but reduce reliability.

### **Common Problems**

* Typographical errors (e.g., `"Delhii"` instead of `"Delhi"`)
* Mixed units (cm vs inches)
* Mixed formats (different date formats)
* Duplicate categories (`"M"` vs `"Male"`)
* String–numeric confusion (`"100"` vs `100`)

### **Solutions**

1. **Standardize units**
2. **Normalize categorical values**
3. **Unify date formats**
4. **Strip whitespace, fix capitalization**
5. **Convert data types properly**

---

# **5. Practical ML Guidelines**

* Always perform **EDA before cleaning**.
* Use **pipelines** to ensure reproducibility.
* Avoid dropping too much data; prefer imputation.
* Validate cleaning choices using cross-validation.
* Document every cleaning rule for auditability (essential in healthcare/finance).

---

# **6. Summary Table**

| Issue           | Detection                 | Treatment                          |
| --------------- | ------------------------- | ---------------------------------- |
| Missing Values  | count NaNs                | mean/median/mode, KNN, model-based |
| Duplicates      | row comparison            | drop duplicates                    |
| Outliers        | Z-score, IQR, Mahalanobis | remove, cap, transform             |
| Inconsistencies | EDA, pattern checks       | normalization, type fixes          |

---

# **Key Takeaways**

* Data cleaning directly affects model accuracy and robustness.
* Missing values require statistical imputation, not blind removal.
* Outliers must be identified using validated statistical methods.
* Consistency and correctness of data formats are essential for pipelines.
* A clean dataset reduces model variance and improves generalization.

---

