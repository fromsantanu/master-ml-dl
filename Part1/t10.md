# **Lesson 10 — Encoding Categorical Data**

Machine learning algorithms require **numerical inputs**.
Categorical features—such as city, color, product type, or disease category—must therefore be converted into numeric representations without distorting their semantics.

This lesson covers the main encoding techniques:

1. **Label Encoding**
2. **One-Hot Encoding**
3. **Ordinal Encoding**
4. **Target Encoding**
5. **Frequency Encoding**
6. **Binary Encoding**
7. When to use which method

---

# **1. Label Encoding**

Maps each category to a unique integer.

Given categories:
[
C = {c_1, c_2, \dots, c_k}
]

Define a mapping:
[
\text{enc}(c_j) = j-1
]

### **Example**

Cities = {Delhi, Mumbai, Kolkata}

| City    | Encoded |
| ------- | ------- |
| Delhi   | 0       |
| Mumbai  | 1       |
| Kolkata | 2       |

### **Pros**

* Very compact
* Useful for tree-based models

### **Cons**

* Imposes **artificial order** → not suitable for linear models, SVM, KNN

---

# **2. One-Hot Encoding (OHE)**

Creates a binary vector of size (k); exactly one element is 1.

For category (c_j), output vector:

[
\mathbf{e}_j = [0, \dots, 0, 1, 0, \dots, 0]
]

### **Example**

Cities = {Delhi, Mumbai, Kolkata}

| City    | Delhi | Mumbai | Kolkata |
| ------- | ----- | ------ | ------- |
| Delhi   | 1     | 0      | 0       |
| Mumbai  | 0     | 1      | 0       |
| Kolkata | 0     | 0      | 1       |

### **Pros**

* No false ordinal relationship
* Good for linear models, logistic regression, SVM

### **Cons**

* High dimensionality if categories are large
* Sparse representation

---

# **3. Ordinal Encoding**

Used when categories have **inherent order**.

Let categories follow:

[
c_1 < c_2 < \dots < c_k
]

Encode:

[
\text{enc}(c_j) = j
]

### **Example**

Sizes: {Small < Medium < Large}

| Size   | Encoded |
| ------ | ------- |
| Small  | 1       |
| Medium | 2       |
| Large  | 3       |

### **Use Case**

* Ordered categories (education level, satisfaction ratings)

### **Avoid for**

* Nominal categorical variables without order

---

# **4. Target Encoding**

Replace each category with the **mean target value** for that category.

For category ( c_j ):

[
\text{enc}(c_j) = \mathbb{E}[Y \mid X=c_j]
]

### **Example**

Dataset (binary classification):

| City    | Bought (Y) |
| ------- | ---------- |
| Delhi   | 1          |
| Delhi   | 0          |
| Mumbai  | 1          |
| Kolkata | 0          |
| Kolkata | 0          |

Compute:

* Delhi: ( (1+0)/2 = 0.5 )
* Mumbai: ( (1)/1 = 1.0 )
* Kolkata: ( (0+0)/2 = 0.0 )

### **Pros**

* Works very well for high-cardinality categorical columns
* Compact numerical representation

### **Cons**

* Risk of **overfitting**
* Must use **cross-validation encoding** to avoid leakage

---

# **5. Frequency Encoding**

Encode category by its relative frequency:

[
\text{enc}(c_j) = \frac{\text{count}(c_j)}{N}
]

### **Example**

Cities: [Delhi, Delhi, Mumbai, Kolkata]

* Delhi: 2/4 = 0.5
* Mumbai: 1/4 = 0.25
* Kolkata: 1/4 = 0.25

### **Pros**

* Very compact
* Preserves distribution signal
* Suitable for tree models

### **Cons**

* Not useful for linear models where categories need orthogonality

---

# **6. Binary Encoding**

Combines hashing + bit-level representation.
Useful for **high-cardinality** variables (e.g., thousands of categories).

Steps:

1. Map category to integer (j)
2. Convert (j) to binary
3. Expand into binary columns

### **Example**

Category → integer → binary:

| Category | Integer | Binary |
| -------- | ------- | ------ |
| A        | 1       | 01     |
| B        | 2       | 10     |
| C        | 3       | 11     |

### **Pros**

* Reduces dimensionality compared to one-hot
* Works for sparse high-cardinality features

---

# **7. When to Use Which Encoding? (Decision Table)**

| Categorical Type          | Examples                 | Best Encoding             |
| ------------------------- | ------------------------ | ------------------------- |
| Nominal (no order), small | Colors, cities           | One-hot                   |
| Ordinal (ordered)         | Sizes, education         | Ordinal encoding          |
| High-cardinality          | Product IDs, URLs        | Target, Frequency, Binary |
| Tree-based models         | Random Forest, XGBoost   | Label/Frequency/Target    |
| Linear models             | Logistic Regression, SVM | One-hot                   |
| Text categories           | Tags                     | One-hot or frequency      |

---

# **8. Encoding and Data Leakage**

For all target-dependent encodings (target, leave-one-out, smoothing):

**Fit encoders ONLY on training data.**

Incorrect:
[
\text{fit}(X_{\text{train}} + X_{\text{test}})
]

Correct:
[
\text{fit}(X_{\text{train}}), \quad \text{transform}(X_{\text{test}})
]

Otherwise, test-set information leaks into training → invalid evaluation.

---

# **Summary**

| Encoding  | Formula                        | Ideal Use          | Notes                    |          |
| --------- | ------------------------------ | ------------------ | ------------------------ | -------- |
| Label     | (c_j \rightarrow j)            | Trees              | Imposes order            |          |
| One-hot   | (\mathbf{e}_j)                 | Linear models      | High dimension           |          |
| Ordinal   | (c_j \rightarrow j) with order | Ordered categories | Preserves order          |          |
| Target    | (\mathbb{E}[Y                  | c_j])              | High-cardinality         | Needs CV |
| Frequency | count/N                        | Large cardinality  | Simple and effective     |          |
| Binary    | Integer → Binary               | High-cardinality   | Lower dimension than OHE |          |

---

# **Key Takeaways**

* Categorical data must be encoded for ML models.
* The encoding choice significantly affects model accuracy and training stability.
* Target and frequency encoding are powerful for high-cardinality columns.
* One-hot encoding is the safest for linear models.
* Avoid data leakage by fitting encoders only on training data.

---
