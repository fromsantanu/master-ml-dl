# **Lesson 14 — Bias vs Variance**

*Fundamental Generalization Trade-off in Machine Learning*

Bias–Variance decomposition explains **why models make errors** and how model complexity affects generalization.

This lesson covers:

1. Prediction error decomposition
2. Bias
3. Variance
4. Irreducible noise
5. Examples with graphs and formulas
6. Practical implications

---

# **1. Prediction Error Decomposition**

For a regression problem:

* True function:
  [
  Y = f(X) + \varepsilon
  ]
* Noise term:
  [
  \mathbb{E}[\varepsilon] = 0,\quad \text{Var}(\varepsilon)=\sigma^2
  ]

Given an estimator (\hat{f}(x)), the expected squared prediction error is:

[
\mathbb{E}\big[(Y - \hat{f}(x))^2\big]
]

The **Bias–Variance decomposition**:

[
\mathbb{E}\big[(Y - \hat{f}(x))^2\big]
= \underbrace{(\mathbb{E}[\hat{f}(x)] - f(x))^2}_{\text{Bias}^2}

* \underbrace{\mathbb{E}\big[(\hat{f}(x) - \mathbb{E}[\hat{f}(x)])^2\big]}_{\text{Variance}}
* \underbrace{\sigma^2}_{\text{Irreducible noise}}
  ]

---

# **2. Bias (Model Simplicity Error)**

Bias measures how far the model’s **average prediction** is from the true function.

### **Definition**

[
\text{Bias}(x) = \mathbb{E}[\hat{f}(x)] - f(x)
]

* High bias → model is **too simple**
* Underfits the data
* Fails to capture true patterns

### **Characteristics of High-Bias Models**

* Linear model fitted to nonlinear data
* Small decision tree
* Very strong regularization

---

# **3. Variance (Model Sensitivity Error)**

Variance measures how much predictions **change** across different training sets.

### **Definition**

[
\text{Variance}(x) = \mathbb{E}\big[(\hat{f}(x) - \mathbb{E}[\hat{f}(x)])^2\big]
]

* High variance → model is **too sensitive**
* Overfits noise in training data
* Poor generalization

### **Characteristics of High-Variance Models**

* Deep decision trees
* 1-NN classifier
* Models with too many parameters (e.g., overly wide neural networks)

---

# **4. Irreducible Noise**

[
\text{Noise} = \sigma^2
]

Noise is inherent randomness in the data.
No model can reduce this component.

Examples:

* Sensor errors
* Human judgment inconsistencies
* Measurement precision limits

---

# **5. Graphical Interpretation**

### **Bias–Variance Trade-off Curve**

* As model complexity increases:

  * Bias ↓
  * Variance ↑

Generalization error:

```
Generalization Error
│     Bias^2
│  \        
│   \____
│        \      Variance
│         \____/
│───────────────→ Model Complexity
```

Optimal complexity occurs at the minimum of the combined curve.

---

# **6. Numerical Example**

Consider true function:

[
f(x) = x^2
]

We fit three models:

---

### **A. High Bias — Linear Model**

[
\hat{f}(x) = ax + b
]

* Cannot capture curvature
* Predictions systematically off
* → High Bias, Low Variance

---

### **B. Good Fit — Quadratic Model**

[
\hat{f}(x) = ax^2 + bx + c
]

* Captures true shape
* → Low Bias, Moderate Variance

---

### **C. High Variance — 10-degree Polynomial**

[
\hat{f}(x) = \sum_{k=0}^{10} a_k x^k
]

* Fits training noise exactly
* Curve oscillates heavily
* → Low Bias, High Variance

---

# **7. Classification Example (Intuition)**

Dataset: Two classes separated by a simple boundary.

### **High Bias Model**

* Logistic regression
* Straight line boundary
* Misses complex regions
* Underfits

### **High Variance Model**

* k-NN with (k=1)
* Boundary follows noise points
* Overfits

### **Balanced Model**

* k-NN with moderate (k)
* Good trade-off

---

# **8. Practical ML Implications**

### **High Bias → Underfitting**

Model too simple.

Fix via:

* Increase model complexity
* Add more features
* Reduce regularization
* Train longer (NNs)

---

### **High Variance → Overfitting**

Model too complex.

Fix via:

* Reduce model complexity
* Add regularization (L1/L2/dropout)
* Use early stopping
* Collect more data
* Use cross-validation
* Use ensemble models (bagging reduces variance)

---

# **9. Summary Table**

| Issue            | Bias | Variance | Behavior                   | Fix                  |
| ---------------- | ---- | -------- | -------------------------- | -------------------- |
| **Underfitting** | High | Low      | Poor training accuracy     | More complexity      |
| **Overfitting**  | Low  | High     | Poor test accuracy         | Regularize, simplify |
| **Good Fit**     | Low  | Low      | Good train + test accuracy | Ideal                |

---

# **Key Takeaways**

* Bias = error from oversimplification
* Variance = error from sensitivity to data
* Total error = Bias² + Variance + Noise
* Perfect generalization requires balancing the two
* The Bias–Variance trade-off guides model selection, regularization, and architecture design

---


