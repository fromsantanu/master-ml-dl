# **Lesson 7 — Data Collection & Sampling (Engineering + Statistical Perspective)**

A Machine Learning system is only as good as the **data** it learns from.
Data collection and sampling define the statistical quality of your dataset and determine the model’s ability to generalize.

This lesson covers:

1. Data collection sources and requirements
2. Sampling methods
3. Statistical properties of good samples
4. Practical ML considerations

---

# **1. Data Collection**

Data collection is the process of gathering raw observations from one or more sources.

## **1.1 Sources of Data**

| Source Type        | Examples                                 | Structure                  |
| ------------------ | ---------------------------------------- | -------------------------- |
| **Databases**      | SQL tables, ERP systems, medical records | Structured                 |
| **APIs**           | weather APIs, financial APIs             | Semi-structured            |
| **User-generated** | forms, surveys, logs                     | Structured/semi-structured |
| **Sensors / IoT**  | temperature, GPS, accelerometers         | Time-series                |
| **Web scraping**   | product prices, article text             | Unstructured               |
| **Files**          | CSV, JSON, Parquet                       | Structured/semi-structured |

---

# **1.2 Data Requirements**

Good ML data must satisfy:

1. **Relevance**
   Data must represent features influencing the target variable.

2. **Accuracy**
   Errors introduce noise → increases variance.

3. **Completeness**
   Missing key fields reduces model reliability.

4. **Consistency**
   No conflicting entries or incompatible formats.

5. **Sufficient volume**
   Amount must exceed model complexity to avoid overfitting.

---

# **2. Sampling**

Sampling means selecting a **subset** of a population to estimate characteristics or train ML models.

Let the population be ( \mathcal{P} = {x_1, \ldots, x_N} ).
A sample ( \mathcal{S} = {x_{i_1}, \ldots, x_{i_n}} \subset \mathcal{P} ) is chosen such that:

[
n \ll N \quad \text{and} \quad \mathcal{S} \text{ is representative of } \mathcal{P}.
]

A representative sample ensures:

[
\mathbb{E}[\hat{\theta}] = \theta
]
(where ( \hat{\theta} ) is the estimated statistic).

---

# **3. Sampling Techniques**

## **3.1 Simple Random Sampling (SRS)**

Every point has equal probability:

[
P(\text{select } x_i) = \frac{1}{N}
]

Implementation:

* Random number generator
* Uniform selection without bias

**Use case:** unbiased estimation, when population is homogeneous.

---

## **3.2 Stratified Sampling**

Data is divided into strata (groups) based on a categorical variable.
Sampling is performed within each stratum.

Let strata be ( S_1, S_2, \ldots, S_k ).

Sample size in each stratum:

[
n_j = \frac{|S_j|}{N} \cdot n
]

**Use case:** heterogeneous population where each subgroup must be represented.

**Examples:**

* Gender-balanced datasets
* Class-balanced classification tasks
* Age groups in medical data

---

## **3.3 Systematic Sampling**

Choose every ( k^{th} ) element from ordered population:

[
k = \frac{N}{n}
]

Simple and effective for time-ordered or spatial data.

**Example:** selecting every 10th log entry from a server.

---

## **3.4 Cluster Sampling**

Population divided into clusters; entire clusters are sampled.

Useful when:

* Population is large
* Sampling individuals is expensive

Example:

* Sampling entire schools instead of individual students.

---

## **3.5 Reservoir Sampling (Streaming)**

For streaming/online data where (N) is unknown:

Maintain a reservoir of size (k).
For each new element (i) (indexed from 1):

```
if i ≤ k:
    store in reservoir
else:
    with probability (k / i):
        replace a random element in reservoir
```

Guarantees uniform probability over the entire stream.

---

# **4. Statistical Requirements for a Good Sample**

A good sample must satisfy:

### **4.1 Representativeness**

[
\mathbb{E}[\hat{\mu}] = \mu \quad\text{and}\quad \text{Var}(\hat{\mu}) \text{ minimized}
]

### **4.2 No sampling bias**

No systematic overrepresentation of a subgroup.

### **4.3 Adequate sample size**

Variance of estimates decreases with sample size:

[
\text{Var}(\hat{\mu}) = \frac{\sigma^2}{n}
]

Hence, doubling the sample size reduces variance by half.

### **4.4 Class balance (ML-specific)**

For classification, imbalanced samples harm performance:

[
P(Y=1) \approx P(Y=0)
]

Techniques like **oversampling**, **undersampling**, or **synthetic sampling (SMOTE)** address imbalance.

---

# **5. Practical ML Considerations**

### **5.1 Avoid data leakage**

Training samples must not include future information.

### **5.2 Temporal sampling (time-series)**

Random sampling is invalid for time-series.
Use chronological split:

[
\text{Train: first 70% of time}, \quad \text{Test: later 30%}
]

### **5.3 Avoid duplicate records**

Duplicated samples overweight certain patterns → bias in training.

### **5.4 Sampling for cross-validation**

In k-fold cross-validation:

[
\text{Each fold must represent the whole distribution.}
]

Hence *stratified sampling* is often used.

---

# **6. Summary Table**

| Technique     | Probability Model          | Advantages             | Limitations               |
| ------------- | -------------------------- | ---------------------- | ------------------------- |
| Simple Random | (1/N) each                 | Unbiased, simple       | May miss minorities       |
| Stratified    | proportional by group      | Preserves group ratios | Requires known strata     |
| Systematic    | every (k)-th item          | Fast, easy             | Patterns in data may bias |
| Cluster       | sample whole clusters      | Low cost               | High variance             |
| Reservoir     | streaming uniform sampling | Online friendly        | Slightly complex          |

---

# **Key Takeaways**

* Data collection must ensure **accuracy, completeness, and consistency**.
* Sampling is essential when working with large populations or limited resources.
* **Stratified sampling** is critical for ML tasks with class imbalance.
* A statistically sound sample improves model generalization.
* Poor sampling introduces **bias**, increasing ML error and reducing reliability.

---

