# **Lesson 1 — What is Machine Learning?**

Machine Learning (ML) is a field of study focused on designing algorithms that **learn patterns from data** and **make predictions** without being explicitly programmed with rules.

Formally, Arthur Samuel (1959) defined ML as:

> *“A field of study that gives computers the ability to learn without being explicitly programmed.”*

In modern terms:

---

## **1. Formal Definition**

Let

* ( X ) = input features (data)
* ( Y ) = target variable (what we want to predict)
* ( f_{\theta} ) = model with parameters ( \theta )

Machine learning attempts to approximate an unknown function:

[
Y = f(X)
]

by finding a model ( \hat{f}_{\theta} ) such that:

[
\hat{Y} = \hat{f}_{\theta}(X)
]

The goal is to choose parameters ( \theta ) that minimize the prediction error.

---

## **2. Core Concept**

ML systems **learn patterns** by analyzing examples.

* In supervised learning, the algorithm sees pairs ((X_i, Y_i)).
* In unsupervised learning, the algorithm sees only (X_i) and must discover structure.
* In reinforcement learning, the algorithm interacts with an environment and receives rewards.

Unlike traditional programming:

| Traditional Programming | Machine Learning            |
| ----------------------- | --------------------------- |
| Rules + Data → Output   | Data + Output → Learn Rules |

---

## **3. What Does “Learning” Mean Mathematically?**

Learning = minimizing a **loss function** that quantifies prediction error.

Given a dataset
[
\mathcal{D} = {(X_i, Y_i)}_{i=1}^n
]

The objective is:

[
\theta^* = \arg\min_{\theta} ; \frac{1}{n} \sum_{i=1}^n L\big(\hat{f}_{\theta}(X_i), Y_i\big)
]

Where:

* ( L(\cdot) ) = loss function

  * e.g., Mean Squared Error (regression)
  * e.g., Cross Entropy (classification)

This optimization is usually done using **gradient descent** or its variants.

---

## **4. Example (Simple Numerical Illustration)**

Suppose we have:

| Hours Studied (X) | Score (Y) |
| ----------------- | --------- |
| 2                 | 50        |
| 4                 | 65        |
| 6                 | 80        |

We want a linear model:

[
\hat{Y} = \theta_0 + \theta_1 X
]

The learning algorithm finds values of (\theta_0, \theta_1) that minimize:

[
\text{MSE} = \frac{1}{3}\sum_{i=1}^3 (Y_i - \hat{Y}_i)^2
]

After training, you may get:

[
\hat{Y} = 40 + 6.5X
]

This model can now **predict unseen values**, e.g., for 5 hours studied:

[
\hat{Y} = 40 + 6.5(5) = 72.5
]

---

## **5. Key Components of any ML System**

### **1. Data**

* Features: (X)
* Labels/Targets: (Y)

### **2. Model**

A mathematical function: linear regression, decision tree, neural network, etc.

### **3. Loss Function**

Measures error.

### **4. Optimization Algorithm**

Updates parameters to reduce loss (e.g., gradient descent).

### **5. Evaluation**

Checking model performance on unseen data.

---

## **6. Types of Machine Learning**

* **Supervised Learning**
  Learn a mapping (X \rightarrow Y) using labeled data.

* **Unsupervised Learning**
  Discover patterns or structure from unlabeled data.

* **Reinforcement Learning**
  Learn actions that maximize cumulative reward in an environment.

---

## **7. Why Machine Learning Works**

ML succeeds because many real-world problems are functions of observable features:

* House price ≈ f(size, location, rooms)
* Spam detection ≈ f(email text)
* Diagnosis ≈ f(symptoms, tests)

Although we don’t know the function exactly, ML can approximate it using data.

---

## **Summary**

Machine Learning is the process of:

1. **Choosing a model** ( \hat{f}_{\theta} )
2. **Feeding it data** ((X, Y))
3. **Optimizing parameters** (\theta) to minimize a loss
4. **Generalizing to new, unseen data**

It combines statistical modeling, optimization, and algorithmic design to build systems that learn automatically.

---
