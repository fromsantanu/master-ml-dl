# **Lesson 2 — Types of Machine Learning (Supervised, Unsupervised, Reinforcement)**

Machine Learning systems are generally categorized into three major types, based on the nature of the data available and the goal of learning:

1. **Supervised Learning**
2. **Unsupervised Learning**
3. **Reinforcement Learning**

Each type has a distinct mathematical structure and objective function.

---

# **1. Supervised Learning**

## **Definition**

We are given a dataset with **input–output pairs**:
[
\mathcal{D} = {(X_i, Y_i)}_{i=1}^n
]

Goal: learn a function
[
\hat{f}: X \rightarrow Y
]
that predicts (Y) from (X).

This is analogous to regression or classification tasks.

---

## **Mathematical Formulation**

We choose a hypothesis class (model) ( \hat{f}_{\theta} ) parameterized by (\theta) and minimize the empirical risk:

[
\theta^* = \arg\min_{\theta}\frac{1}{n}\sum_{i=1}^n L\big(\hat{f}_{\theta}(X_i), Y_i\big)
]

Where:

* (L) is a loss function
* For regression: (L = (Y - \hat{Y})^2)
* For classification: cross-entropy loss

---

## **Examples**

| Task                   | Input (X)      | Output (Y)      | Model Type     |
| ---------------------- | -------------- | --------------- | -------------- |
| House Price Prediction | Size, location | Price           | Regression     |
| Email Spam Detection   | Email text     | Spam / Not spam | Classification |
| Medical Diagnosis      | Symptoms       | Disease label   | Classification |

---

## **Numeric Example**

Dataset:

| (X) (Hours studied) | (Y) (Score) |
| ------------------- | ----------- |
| 2                   | 50          |
| 4                   | 63          |
| 6                   | 78          |

Model:
[
\hat{Y} = \theta_0 + \theta_1 X
]

Learning = minimizing MSE.

---

# **2. Unsupervised Learning**

## **Definition**

Dataset contains only **inputs**, no outputs:

[
\mathcal{D} = {X_i}_{i=1}^n
]

Goal: discover structure, patterns, or representations.

---

## **Major Tasks**

* **Clustering** (grouping similar data)
* **Dimensionality Reduction** (compressing data)
* **Density Estimation**

---

## **Mathematical Formulation**

### **Clustering (e.g., K-means)**

Find cluster centers ( {\mu_k}_{k=1}^K ) that minimize:

[
\sum_{i=1}^n \min_k , \lVert X_i - \mu_k \rVert^2
]

### **Dimensionality Reduction (e.g., PCA)**

Maximize variance captured by projection:

[
\max_{W} ; \text{Var}(W^\top X)
]

---

## **Examples**

| Task                  | Description                          |
| --------------------- | ------------------------------------ |
| Customer Segmentation | Group customers by spending patterns |
| Image Compression     | Reduce dimensions using PCA          |
| Topic Modeling        | Discover themes in text documents    |

---

## **Simple Numerical Example (Clustering)**

Data points: (X = {1, 2, 10, 11})

A 2-cluster solution might be:

* Cluster 1: {1, 2}
* Cluster 2: {10, 11}

The algorithm discovers these groups **without labels**.

---

# **3. Reinforcement Learning (RL)**

## **Definition**

The algorithm interacts with an environment and learns a **policy** to maximize long-term reward.

An RL problem is defined by a Markov Decision Process (MDP):

[
(S, A, P, R, \gamma)
]

where:

* (S): states
* (A): actions
* (P(s'|s,a)): transition probability
* (R(s,a)): reward
* (\gamma): discount factor (0–1)

The agent chooses actions using a policy ( \pi(a|s) ).

---

## **Objective**

Maximize expected return:

[
\max_{\pi} ; \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t R(s_t, a_t)\right]
]

---

## **Examples**

| Application            | Description                        |
| ---------------------- | ---------------------------------- |
| Game Playing           | Chess, Go, Atari                   |
| Robotics               | Navigation, locomotion             |
| Self-Driving Vehicles  | Learn driving policies             |
| Recommendation Systems | Optimize long-term user engagement |

---

## **Simple Numerical Example**

State: robot at position 0
Actions: move +1 or −1
Reward: +10 when reaching position 5

The agent tries various moves and learns a policy (\pi(a|s)) that brings it to state 5 with maximum expected reward.

---

# **Summary Table**

| Learning Type     | Data Provided   | Goal                  | Mathematical Objective                     |
| ----------------- | --------------- | --------------------- | ------------------------------------------ |
| **Supervised**    | ((X, Y)) pairs  | Predict output        | Minimize loss (L(Y, \hat{Y}))              |
| **Unsupervised**  | Only (X)        | Discover structure    | Minimize reconstruction / clustering error |
| **Reinforcement** | States, rewards | Learn optimal actions | Maximize long-term reward                  |

---

# **Key Takeaway**

* **Supervised learning** approximates (f(X) \rightarrow Y) from labeled data.
* **Unsupervised learning** discovers structure from unlabeled data.
* **Reinforcement learning** learns behavior by trial, error, and reward.

Together, these three pillars form the foundation of modern Machine Learning.

---

