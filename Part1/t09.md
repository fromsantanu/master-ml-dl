# **Lesson 9 — Scaling & Normalization**

Most machine learning algorithms assume that features exist on **comparable numerical scales**.
If different features have different ranges (e.g., height in cm vs income in rupees), models can behave poorly.

Scaling and normalization transform features to make them numerically well-behaved for ML algorithms.

This lesson covers:

1. Why scaling is needed
2. Min–Max Scaling
3. Standardization (Z-score scaling)
4. Normalization (L1, L2 vector normalization)
5. When to use each method

---

# **1. Why Scaling and Normalization Are Important**

Let feature values be:

* Height: (150 \text{–} 190)
* Income: (10{,}000 \text{–} 10{,}00{,}000)

Income dominates due to its larger magnitude.

### **Algorithms affected by scale**

* Gradient descent methods
* KNN
* K-means
* PCA
* SVM with RBF kernel
* Neural networks

These algorithms use **distances**, **inner products**, or **gradients**, all sensitive to scale.

### **Scale-invariant algorithms**

* Decision trees
* Random forests
* Gradient boosting trees

These models split based on thresholds and do not rely on feature magnitude.

---

# **2. Min–Max Scaling (Normalization to [0,1])**

Transforms a feature (x) to the range ([0, 1]):

[
x' = \frac{x - x_{\min}}{x_{\max} - x_{\min}}
]

### **Example**

Feature values: ({10, 20, 30})

[
x_{\min}=10, ; x_{\max}=30
]
[
x' = \frac{x - 10}{20}
]

So:

* 10 → 0
* 20 → 0.5
* 30 → 1

### **Pros**

* Preserves shape of distribution
* Keeps values within [0, 1]

### **Cons**

* Very sensitive to **outliers**; a single extreme value distorts the scale

### **Use Cases**

* Neural networks (sigmoid/tanh)
* KNN, K-means
* Distance-based models
* When all features must lie in [0,1]

---

# **3. Standardization (Z-score Scaling)**

Transforms feature to have mean 0 and std = 1:

[
x' = \frac{x - \mu}{\sigma}
]

Where:

* (\mu) = mean of the feature
* (\sigma) = standard deviation

### **Example**

Values: ({10, 12, 14})

[
\mu = 12, \quad \sigma = \sqrt{\frac{(10-12)^2+(12-12)^2+(14-12)^2}{3}} = \sqrt{2.67}=1.63
]

[
10' = \frac{10 - 12}{1.63} \approx -1.23
]
[
12' = 0
]
[
14' = 1.23
]

### **Pros**

* More robust to outliers than min-max
* Makes features comparable
* Works well for algorithms assuming Gaussian-like data

### **Cons**

* Does not bound values to fixed range

### **Use Cases**

* Logistic regression
* Linear regression
* SVM
* PCA
* Neural networks

---

# **4. Normalization (L1 / L2 Vector Normalization)**

Normalization rescales **each sample vector**, not each feature.

Given vector:

[
\mathbf{x} = [x_1, x_2, \dots, x_d]
]

## **L2 Normalization (unit length)**

[
\mathbf{x}' = \frac{\mathbf{x}}{\lVert \mathbf{x} \rVert_2} =
\frac{\mathbf{x}}{\sqrt{\sum_{j=1}^{d} x_j^2}}
]

Used heavily in text vector representations (TF-IDF).

## **L1 Normalization**

[
\mathbf{x}' = \frac{\mathbf{x}}{\lVert \mathbf{x} \rVert_1} =
\frac{\mathbf{x}}{\sum_{j=1}^{d} |x_j|}
]

### **Use Cases**

* NLP feature vectors
* Cosine-similarity models
* K-means on high-dimensional text data

---

# **5. Which Method Should You Use? (Decision Table)**

| Scenario                                              | Recommended Scaling        |
| ----------------------------------------------------- | -------------------------- |
| Features used by distance-based models (KNN, K-means) | Min–Max or Standardization |
| Features roughly Gaussian                             | Standardization            |
| Neural networks (ReLU)                                | Standardization            |
| Neural networks (Sigmoid/Tanh)                        | Min–Max                    |
| Sparse high-dimensional vectors (NLP)                 | L2 Normalization           |
| Presence of strong outliers                           | Standardization > Min–Max  |
| Tree-based models                                     | No scaling needed          |

---

# **6. Practical ML Pipeline Tip**

Always **fit** scalers on training data only:

[
\text{fit on } X_{\text{train}}, \quad \text{transform on } X_{\text{train}}, X_{\text{test}}
]

Never fit on the full dataset — that causes **data leakage**.

---

# **7. Summary**

| Method           | Formula                                    | Output       | Best For                | Notes                        |
| ---------------- | ------------------------------------------ | ------------ | ----------------------- | ---------------------------- |
| Min–Max Scaling  | ( \frac{x - x_{\min}}{x_{\max}-x_{\min}} ) | [0, 1]       | Distance-based models   | Sensitive to outliers        |
| Standardization  | ( \frac{x - \mu}{\sigma} )                 | Mean 0, SD 1 | Linear models, PCA, SVM | Most widely used             |
| L2 Normalization | ( \frac{x}{\lVert x \rVert_2} )            | Unit vector  | Text data               | Works with cosine similarity |

---

# **Key Takeaways**

* Scaling is necessary when feature magnitudes vary significantly.
* Min–Max scaling rescales to [0,1].
* Standardization creates zero-mean, unit-variance features.
* L1/L2 normalization rescales each sample vector.
* Scaling must always be applied **after splitting** to avoid leakage.

---
