# **Lesson 6 — Probability for Machine Learning (Foundations)**

Probability provides the mathematical framework for modeling **uncertainty**, which is fundamental in Machine Learning.
Classification, regression, Bayesian inference, logistic regression, decision trees, and neural networks all rely on probability concepts—explicitly or implicitly.

This lesson introduces core probability ideas used throughout ML.

---

# **1. Sample Space, Events, and Probability**

## **Sample Space**

The set of all possible outcomes.

[
\Omega = {\omega_1, \omega_2, \dots}
]

Example (coin toss):
[
\Omega = {\text{Heads}, \text{Tails}}
]

## **Event**

A subset of the sample space.

[
A \subseteq \Omega
]

Example: Getting Heads → (A = {\text{Heads}})

## **Probability of an Event**

A function (P) mapping events to ([0,1]):

[
P(A) = \frac{\text{number of favorable outcomes}}{\text{total outcomes}}
]

Example (coin):
[
P(\text{Heads}) = \frac{1}{2}
]

---

# **2. Conditional Probability**

Conditional probability measures the likelihood of an event (A) given event (B):

[
P(A|B) = \frac{P(A \cap B)}{P(B)}
]

### **Example**

Let:

* (A): person has fever
* (B): person has flu

If every flu patient has fever:

[
P(A|B) = 1
]

Conditional probability is crucial in ML classification tasks.

---

# **3. Bayes' Rule**

Bayes’ theorem relates conditional probabilities:

[
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
]

This is the basis for **Naive Bayes**, Bayesian networks, probabilistic models, and uncertainty quantification.

### **Example (Medical Testing)**

* Prior probability: (P(\text{Disease}) = 0.01)
* Test sensitivity: (P(\text{Positive}|\text{Disease}) = 0.99)
* Test false positive rate: (P(\text{Positive}|\text{Healthy}) = 0.05)

The posterior probability that a positive test implies disease is:

[
P(\text{Disease}|\text{Positive}) =
\frac{0.99 \cdot 0.01}{0.99\cdot 0.01 + 0.05\cdot 0.99}
\approx 0.166
]

Even with a positive test, probability is only **16.6%**.
This illustrates why probability matters in real ML applications.

---

# **4. Random Variables**

A random variable maps outcomes to real numbers.

[
X: \Omega \rightarrow \mathbb{R}
]

### Types:

* **Discrete** (finite/countable): number of clicks, count of emails.
* **Continuous**: height, blood pressure, sensor readings.

---

# **5. Probability Mass Function (PMF)**

For discrete random variable (X):

[
P(X = x) = p(x)
]

Example:

| X | p(X) |
| - | ---- |
| 0 | 0.2  |
| 1 | 0.5  |
| 2 | 0.3  |

Sum must equal 1.

---

# **6. Probability Density Function (PDF)**

For continuous random variables:

[
P(a \le X \le b) = \int_a^b f(x),dx
]

Example: Normal distribution, exponential distribution.

---

# **7. Expectation and Variance**

## **Expectation (Mean)**

Discrete:
[
\mathbb{E}[X] = \sum_x x , p(x)
]

Continuous:
[
\mathbb{E}[X] = \int_{-\infty}^{\infty} x f(x),dx
]

## **Variance**

[
\text{Var}(X) = \mathbb{E}[(X - \mathbb{E}[X])^2]
]

High variance → high uncertainty.

---

# **8. Joint and Marginal Probability**

For two random variables (X, Y):

## **Joint probability**

[
P(X=x, Y=y)
]

Models dependency.

## **Marginal probability**

[
P(X=x) = \sum_y P(X=x, Y=y)
]
(discrete)

Used in feature independence assumptions (e.g., Naive Bayes).

---

# **9. Independence**

Two events are independent if:

[
P(A \cap B) = P(A)P(B)
]

Two variables (X, Y) independent if:

[
P(X, Y) = P(X)P(Y)
]

Many ML models assume independence (though often partially violated).

---

# **10. Why Probability is Critical in ML**

Probability appears in almost every ML technique:

| ML Method               | Probability Role                   |                 |
| ----------------------- | ---------------------------------- | --------------- |
| Logistic Regression     | Outputs (P(Y=1                     | X)) via sigmoid |
| Naive Bayes             | Direct use of Bayes’ theorem       |                 |
| Gaussian Mixture Models | Probabilistic clustering           |                 |
| Hidden Markov Models    | Sequential probability             |                 |
| Neural Networks         | Softmax → probability distribution |                 |
| Decision Theory         | Expected risk minimization         |                 |

Even models that look “non-probabilistic” (e.g., SVM, trees) rely on statistical assumptions.

---

# **Summary**

Key probability tools required in ML:

| Concept                 | Formula                 | Usage                 |                |
| ----------------------- | ----------------------- | --------------------- | -------------- |
| Conditional Probability | (P(A                    | B))                   | Classification |
| Bayes’ Rule             | (\frac{P(B              | A)P(A)}{P(B)})        | Bayesian ML    |
| Expectation             | (\mathbb{E}[X])         | Model analysis        |                |
| Variance                | (\mathbb{E}[(X-\mu)^2]) | Uncertainty           |                |
| Joint/Marginals         | sums or integrals       | Multivariate analysis |                |

Probability provides the mathematical foundation for uncertainty modeling, prediction confidence, and decision-making in ML.

---
