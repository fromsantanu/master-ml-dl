# **Lesson 13 — Train/Test Split**

*Generalization, Data Partitioning, and Evaluation Foundations*

The **train/test split** is the standard method to evaluate whether a machine learning model generalizes well to unseen data. Instead of training and evaluating on the same dataset—which leads to overfitting—we partition the dataset into **training** and **testing** subsets.

This lesson provides the formal motivation, mathematical basis, and best practices.

---

# **1. Why Train/Test Split? (Generalization Theory)**

Let:

* (\mathcal{D} = {(X_i, Y_i)}_{i=1}^N) be the dataset sampled i.i.d. from unknown distribution (P(X, Y))
* The model learns a function (\hat{f}_\theta)

The goal is to minimize **expected risk**:

[
R(\theta) = \mathbb{E}*{(X,Y)\sim P} \left[ L\big(\hat{f}*\theta(X), Y \big)\right]
]

But (P) is unknown. We only have the sample (\mathcal{D}).

We approximate expected risk using:

* **Training error:**
  [
  R_{\text{train}}(\theta) = \frac{1}{|\mathcal{D}*{\text{train}}|} \sum L(\hat{f}*\theta(X_i), Y_i)
  ]

* **Test error:**
  [
  R_{\text{test}}(\theta) = \frac{1}{|\mathcal{D}*{\text{test}}|} \sum L(\hat{f}*\theta(X_i), Y_i)
  ]

Because (\mathcal{D}*{\text{test}}) is **independent** of training,
(R*{\text{test}}) is an unbiased estimator of generalization error.

---

# **2. How the Dataset is Split**

Typically:

* **Training set:** 70–80%
* **Test set:** 20–30%

Formally:

[
\mathcal{D}*{\text{train}} \cap \mathcal{D}*{\text{test}} = \emptyset
]
[
\mathcal{D}*{\text{train}} \cup \mathcal{D}*{\text{test}} = \mathcal{D}
]

### Common ratios:

* **80/20 split**
* **75/25 split**
* **70/30 split**

The split may be **stratified** for classification (to maintain class balance).

---

# **3. Why Not Train and Test on the Same Data?**

If a model is evaluated on training data:

* It appears artificially good
* It may simply **memorize** patterns (overfitting)

Expected behavior:

[
R_{\text{train}}(\theta) \ll R_{\text{test}}(\theta)
]

Model generalizes poorly if the gap is large.

---

# **4. Visual Interpretation**

A model trained only on training data should generalize to new points:

```
Data Distribution (P)
│
├── Training Set → fit model
└── Test Set → estimate generalization error
```

The test set simulates “future unseen data.”

---

# **5. Mathematical Example**

Suppose dataset has 1000 samples.
Using an 80/20 split:

[
|\mathcal{D}*{\text{train}}| = 800, \quad |\mathcal{D}*{\text{test}}| = 200
]

Train a simple linear regression model:

[
\hat{Y} = \theta_0 + \theta_1 X
]

Compute:

* Training MSE
  [
  \text{MSE}_{\text{train}} = \frac{1}{800}\sum (Y_i - \hat{Y_i})^2
  ]

* Test MSE
  [
  \text{MSE}_{\text{test}} = \frac{1}{200}\sum (Y_i - \hat{Y_i})^2
  ]

If:

* Train MSE = 20
* Test MSE = 24
  → Good generalization.

If:

* Train MSE = 5
* Test MSE = 150
  → Heavy overfitting.

---

# **6. Important Variants**

## **6.1 Stratified Split (for Classification)**

Ensures class proportions remain constant.

For classes (C_1, C_2, \ldots, C_k):

[
\frac{|C_j|*{\text{train}}}{|\mathcal{D}*{\text{train}}|} =
\frac{|C_j|*{\text{test}}}{|\mathcal{D}*{\text{test}}|}
]

Useful for imbalanced data.

---

## **6.2 Shuffle Split**

Randomly shuffle data before splitting to ensure iid assumption.

---

## **6.3 Time-Series Split**

Time-series data **must not be shuffled**.

Instead:

[
\text{Train} = \text{old data}, \quad \text{Test} = \text{recent data}
]

Example:

* Train: Jan–Sep
* Test: Oct–Dec

Preserves temporal dependency and avoids leakage.

---

# **7. Practical Guidelines**

1. **Always split before preprocessing.**
   Fit scalers, encoders on training data only.

2. **Use stratification for classification.**

3. **Ensure no duplicates leak across sets.**

4. **Use the test set only once** — do not repeatedly tune based on test results.

5. For small datasets, use **cross-validation** instead of a single split.

---

# **8. Summary Table**

| Component          | Role                                                  |
| ------------------ | ----------------------------------------------------- |
| **Training Set**   | Used to estimate model parameters (\theta)            |
| **Validation Set** | Used for hyperparameter tuning (optional if using CV) |
| **Test Set**       | Used to estimate final generalization error           |

---

# **Key Takeaways**

* Train/test split prevents overfitting and gives an unbiased estimate of generalization performance.
* Stratification and proper data handling (no leakage) are essential.
* Time-series data requires chronological splitting.
* Use cross-validation when data is limited.

---

