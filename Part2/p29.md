# **Lesson 29 â€” ROCâ€“AUC & PR Curves**

*Explained in very simple, everyday language*

---

### ğŸŒŸ **Why Do We Need ROC and PR Curves?**

When a model predicts **categories** (YES/NO),
it usually gives a **probability**, like:

* 0.90 â†’ 90% chance of â€œYESâ€
* 0.30 â†’ 30% chance of â€œYESâ€

But now we must decide:
**At what cut-off do we say YES?**
Is it 0.5? 0.6? 0.7?

Different cut-offs give different results.

ğŸ‘‰ **ROC and PR curves help us understand how good the model is across ALL possible cut-offs**, not just one.

This gives a complete picture of performance.

---

## **Part 1 â€” ROC Curve (Receiver Operating Characteristic)**

### ğŸŒŸ **What does ROC tell us?**

ROC curve shows how well the model separates:

* **Positive class** (YES)
* **Negative class** (NO)

It does this by checking:

* **True Positive Rate** (Recall)
* **False Positive Rate** (False alarms)

For many thresholds (0.1, 0.2, 0.3â€¦).

---

### ğŸ§’ **Simple Idea**

Imagine you are a security guard checking visitors.

If you tighten checking (higher threshold):

* You will catch more intruders (high recall)
* But you may also stop more innocent people (high false positives)

If you loosen checking (lower threshold):

* You may let intruders go (low recall)
* But you disturb fewer innocents (low false positives)

ROC curve checks many such combinations
and draws a graph.

---

## ğŸ“ˆ **What ROC Curve Looks Like**

```
TPR (Recall)  |
1.0           |         â— Best models stay here
              |       /
0.5           |     /
              |   /
0.0           |__/________________
                  0.0   FPR    1.0
```

* **Left-top corner** = excellent model
* **Diagonal line** = useless model (random guessing)
* **Below diagonal** = worse than random

---

## ğŸ¯ **AUC (Area Under Curve)**

AUC is the **score** of the ROC curve.
Higher AUC = better model.

* **AUC 0.9â€“1.0** â†’ Excellent
* **AUC 0.8â€“0.9** â†’ Very good
* **AUC 0.7â€“0.8** â†’ Good
* **AUC 0.6â€“0.7** â†’ Weak
* **AUC < 0.6** â†’ Poor

ğŸ‘‰ AUC shows how well the model can separate the two classes.

---

## â³ **When ROC-AUC Works Best**

* When **classes are balanced**
  (almost equal number of YES and NO)

Example:
Spam vs Not spam (balanced dataset).

---

---

# **Part 2 â€” PR Curve (Precisionâ€“Recall Curve)**

### ğŸŒŸ Why PR Curve?

PR curve focuses on:

* **Precision** â†’ among predicted YES, how many were correct
* **Recall** â†’ out of all actual YES, how many were found

PR curve is mainly used when the positive class is **rare**.

---

### ğŸ§’ **Simple Example**

Imagine:

* 1 out of 100 transactions is fraud
  â†’ Fraud is **rare**

If a model predicts â€œNO fraudâ€ for all 100:

* Accuracy = 99%
* But it missed the 1 real fraud â†’ useless!

Precision and Recall give a **clearer picture** on such problems.

---

## ğŸ“ˆ **What PR Curve Looks Like**

```
Precision  |
1.0        | â—â—â—
0.5        |   â—
0.0        |_____________
             0.0   Recall 1.0
```

### ğŸŒŸ **AUC-PR (Area under PR curve)**

Tells you how well the model performs **when positives are rare**.

---

## ğŸ¯ **When to Use PR Curve?**

Use PR curve when:

* Data is **imbalanced**
* Positive class is **rare**

Examples:

* Fraud detection
* Cancer detection
* Rare disease detection
* Fault detection in machines

PR curve gives much more honest results than ROC in these cases.

---

# ğŸŒ± **ROC vs PR Curve (Super Simple Comparison)**

| Situation                       | Best Curve  |
| ------------------------------- | ----------- |
| Classes balanced                | **ROC-AUC** |
| Positive class rare             | **PR-AUC**  |
| Want overall separation ability | **ROC-AUC** |
| Want performance on positives   | **PR-AUC**  |

---

# ğŸ§  **Tiny Example for Clarity**

Imagine 1000 emails:

* 950 normal
* 50 spam

A model says all 1000 are normal:

* Accuracy = 95%
  But it missed all spam emails â†’ terrible model.

ROC may still look okay,
but PR curve will be **very bad**,
showing the true performance.

---

# ğŸ **Summary (Very Simple)**

### **ROC-AUC**

* Measures how well the model separates YES vs NO
* Good when classes are balanced
* AUC close to 1 = excellent model

### **PR Curve**

* Measures precision vs recall
* Best for rare events
* Shows how well the model identifies important cases

Both curves help us understand model performance much better than just accuracy.

---
